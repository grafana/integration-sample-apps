#cloud-config
# Cloud-init configuration for setting up Alloy and Apache Couchbase sample-app

package_update: true
package_upgrade: false

apt:
  sources:
    grafana:
      source: deb https://apt.grafana.com stable main
      keyid: 963FA27710458545
      keyserver: https://apt.grafana.com/gpg.key

# Define required packages
packages:
  - git
  - gpg
  - curl

write_files:
  - owner: root:root
    path: /etc/default/alloy
    content: |
      ## Path:
      ## Description: Grafana Alloy settings
      ## Type:        string
      ## Default:     ""
      ## ServiceRestart: alloy
      #
      # Command line options for Alloy.
      #
      # The configuration file holding the Alloy config.
      CONFIG_FILE="/etc/alloy/config.alloy"

      # User-defined arguments to pass to the run command.
      CUSTOM_ARGS="--stability.level=experimental"

      # Restart on system upgrade. Defaults to true.
      RESTART_ON_UPGRADE=true

  # Couchbase configuration
  - owner: root:root
    path: /etc/alloy/config.alloy
    content: |
      logging {
        level = "debug"
      }
      
      prometheus.exporter.self "alloy_check" { }

      discovery.relabel "alloy_check" {
        targets = prometheus.exporter.self.alloy_check.targets

        rule {
          target_label = "instance"
          replacement  = constants.hostname
        }

        rule {
          target_label = "alloy_hostname"
          replacement  = constants.hostname
        }

        rule {
          target_label = "job"
          replacement  = "integrations/alloy-check"
        }
      }

      prometheus.scrape "alloy_check" {
        targets    = discovery.relabel.alloy_check.output
        forward_to = [prometheus.relabel.alloy_check.receiver]  

        scrape_interval = "60s"
      }

      prometheus.relabel "alloy_check" {
        forward_to = [prometheus.remote_write.metrics_service.receiver]
        rule {
          source_labels = ["__name__"]
          regex         = "(prometheus_target_sync_length_seconds_sum|prometheus_target_scrapes_.*|prometheus_target_interval.*|prometheus_sd_discovered_targets|alloy_build.*|prometheus_remote_write_wal_samples_appended_total|process_start_time_seconds)"
          action        = "keep"
        }
      }

      {% if prom_url -%}
      prometheus.remote_write "metrics_service" {
        endpoint {
          url = "{{ prom_url }}"

          {% if prom_user and prom_pass -%}
          basic_auth {
            username = "{{ prom_user }}"
            password = "{{ prom_pass }}"
          }
          {%- endif %}
        }
      }
      {%- endif %}

      {% if loki_url -%}
      loki.write "grafana_loki" {
        endpoint {
          url = "{{ loki_url }}"

          {% if loki_user and loki_pass -%}
          basic_auth {
            username = "{{ loki_user }}"
            password = "{{ loki_pass }}"
          }
          {%- endif %}
        }
      }

      local.file_match "logs_integrations_integrations_couchbase" {
        path_targets = [{
          __address__       = "localhost",
          __path__         = "/opt/couchbase/var/lib/couchbase/logs/*.log",
          couchbase_cluster = "{{ couchbase_cluster }}",
          instance         = constants.hostname,
          job             = "integrations/couchbase",
        }]
      }

      loki.process "logs_integrations_integrations_couchbase" {
        forward_to = [loki.write.grafana_loki.receiver]

        stage.drop {
          expression = "---"
        }

        stage.multiline {
          firstline     = "\\[(ns_server|couchdb):(error|info),.*\\]"
          max_lines     = 0
          max_wait_time = "3s"
        }
      }

      loki.source.file "logs_integrations_integrations_couchbase" {
        targets    = local.file_match.logs_integrations_integrations_couchbase.targets
        forward_to = [loki.process.logs_integrations_integrations_couchbase.receiver]
      }
      {%- endif %}

      prometheus.scrape "couchbase" {
        targets = [
          { "__address__" = "localhost:8091" },
        ]
        job_name = "integrations/couchbase"
        forward_to = [prometheus.relabel.couchbase.receiver]
        scrape_interval = "10s"
        metrics_path = "/metrics"
        basic_auth {
          username = "admin"
          password = "password"
        }
      }

      prometheus.relabel "couchbase" {
        forward_to = [prometheus.remote_write.metrics_service.receiver]
        
        rule {
          target_label = "instance"
          replacement = constants.hostname
        }
        
        rule {
          target_label = "couchbase_cluster"
          replacement = "{{ couchbase_cluster }}"
        }
      }

  # Loadgen script for couchbase, will initialize the cluster and create a bucket
  # will also run invalid queries in the background to generate some errors and log data
  - path: /tmp/loadgen.sh
    permissions: '0644'
    content: |
      #!/bin/bash

      # Initialize cluster with minimum required memory settings
      echo "Initializing Couchbase cluster..."
      /opt/couchbase/bin/couchbase-cli cluster-init -c 127.0.0.1 \
        --cluster-username admin \
        --cluster-password password \
        --cluster-name sample-app-couchbase-cluster \
        --cluster-ramsize 600 \
        --cluster-index-ramsize 256 \
        --services data,query,index \
        --index-storage-setting default

      # Install sample buckets
      curl -X POST -u admin:password \
        http://localhost:8091/sampleBuckets/install \
        -d '["travel-sample", "beer-sample"]' | jq .

      run_valid_queries() {
        curl -X POST -u admin:password http://localhost:8093/pools/default/buckets/travel-sample/docs \
          -H "Content-Type: application/json" \
          -d '{"type": "test", "value": 1}' || echo "Document creation failed, but continuing..."

        for i in {1..10}; do
          curl -s -u admin:password http://localhost:8093/query/service \
            -H "Content-Type: application/json" \
            -d "{\"statement\": \"SELECT * FROM travel-sample\", \"client_context_id\": \"valid_query_${i}\"}"
        done
      }

      run_invalid_queries() {
        curl -s -u admin:password http://localhost:8093/query/service \
          -H "Content-Type: application/json" \
          -d '{"statement": "SELECT * FROM travel-sample WHERE", "client_context_id": "invalid_query_1"}'

        curl -s -u admin:password http://localhost:8093/query/service \
          -H "Content-Type: application/json" \
          -d '{"statement": "SELECT * FROM nonexistent_bucket", "client_context_id": "invalid_query_2"}'

        curl -s -u admin:password http://localhost:8093/query/service \
          -H "Content-Type: application/json" \
          -d '{"statement": "SELECT nonexistent_field FROM beer-sample", "client_context_id": "invalid_query_3"}'
      }

      # KV operations for beer-sample bucket
      run_kv_set_operations() {
        echo "Running SET operations on beer-sample bucket..."
        
        # Create some test beer documents using N1QL UPSERT
        for i in {1..5}; do
          DOC_ID="test_beer_${i}_$(date +%s)"
          ABV_INT=$(shuf -i 4-12 -n 1)
          ABV_DEC=$(shuf -i 0-9 -n 1)
          
          # Use N1QL UPSERT instead of REST API
          UPSERT_QUERY="{
            \"statement\": \"UPSERT INTO \`beer-sample\` (KEY, VALUE) VALUES ('${DOC_ID}', { 'name': 'Test Ale ${i}', 'type': 'beer', 'brewery_id': 'test_brewery_${i}', 'abv': ${ABV_INT}.${ABV_DEC}, 'description': 'A delicious test ale number ${i}', 'style': 'Test Style', 'category': 'Test Category', 'updated': '$(date -Iseconds)', 'load_test': true })\",
            \"client_context_id\": \"upsert_${DOC_ID}\"
          }"
          
          echo "Upserting document: ${DOC_ID}"
          curl -s -u admin:password http://localhost:8093/query/service \
            -H "Content-Type: application/json" \
            -d "${UPSERT_QUERY}" || echo "UPSERT failed for ${DOC_ID}"
        done
      }

      run_kv_get_operations() {
        echo "Running GET operations on beer-sample bucket..."
        
        # Get some existing beer documents using N1QL SELECT
        SAMPLE_BEER_IDS=("21st_amendment_brewery_cafe" "21st_amendment_brewery_cafe-21a_ipa" "3_fonteinen_brouwerij")
        
        for beer_id in "${SAMPLE_BEER_IDS[@]}"; do
          echo "Getting beer document: ${beer_id}"
          SELECT_QUERY="{
            \"statement\": \"SELECT * FROM \`beer-sample\` USE KEYS ['${beer_id}']\",
            \"client_context_id\": \"select_${beer_id}\"
          }"
          curl -s -u admin:password http://localhost:8093/query/service \
            -H "Content-Type: application/json" \
            -d "${SELECT_QUERY}" || echo "SELECT failed for ${beer_id}"
        done
        
        # Get our test documents using pattern matching
        echo "Getting test documents..."
        TEST_SELECT_QUERY="{
          \"statement\": \"SELECT META().id, * FROM \`beer-sample\` WHERE load_test = true LIMIT 5\",
          \"client_context_id\": \"select_test_docs\"
        }"
        curl -s -u admin:password http://localhost:8093/query/service \
          -H "Content-Type: application/json" \
          -d "${TEST_SELECT_QUERY}" || echo "SELECT failed for test documents"
      }

      run_kv_delete_operations() {
        echo "Running DELETE operations on beer-sample bucket..."
        
        # Delete some of our test documents using N1QL DELETE
        echo "Deleting test documents..."
        DELETE_QUERY="{
          \"statement\": \"DELETE FROM \`beer-sample\` WHERE load_test = true LIMIT 3\",
          \"client_context_id\": \"delete_test_docs\"
        }"
        curl -s -u admin:password http://localhost:8093/query/service \
          -H "Content-Type: application/json" \
          -d "${DELETE_QUERY}" || echo "DELETE failed for test documents"
        
        # Try to delete some non-existent documents to generate errors
        for i in {1..2}; do
          FAKE_DOC_ID="nonexistent_beer_${i}_$(date +%s)"
          echo "Attempting to delete non-existent document: ${FAKE_DOC_ID}"
          DELETE_FAKE_QUERY="{
            \"statement\": \"DELETE FROM \`beer-sample\` USE KEYS ['${FAKE_DOC_ID}']\",
            \"client_context_id\": \"delete_${FAKE_DOC_ID}\"
          }"
          curl -s -u admin:password http://localhost:8093/query/service \
            -H "Content-Type: application/json" \
            -d "${DELETE_FAKE_QUERY}" || echo "DELETE failed for ${FAKE_DOC_ID} (expected - document doesn't exist)"
        done
      }

      # Enhanced function to run all KV operations
      run_kv_operations() {
        echo "=== Running Key-Value operations on beer-sample bucket ==="
        run_kv_set_operations
        sleep 2
        run_kv_get_operations
        sleep 2
        run_kv_delete_operations
        echo "=== KV operations completed ==="
      }

      while true; do
        echo "Running invalid queries..."
        run_invalid_queries
        echo "Running valid queries..."
        run_valid_queries
        echo "Running KV operations..."
        run_kv_operations
        sleep 5
      done

  # Systemd service file for loadgen
  - path: /etc/systemd/system/couchbase-loadgen.service
    permissions: '0644'
    content: |
      [Unit]
      Description=Couchbase Load Generator
      After=network.target couchbase-server.service
      Wants=couchbase-server.service
      StartLimitIntervalSec=0

      [Service]
      Type=simple
      Restart=always
      RestartSec=10
      User=ubuntu
      Group=ubuntu
      ExecStart=/home/ubuntu/loadgen.sh
      WorkingDirectory=/home/ubuntu
      Environment=PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

      [Install]
      WantedBy=multi-user.target

runcmd:
  # Create keyring directory
  - mkdir -p /etc/apt/keyrings/
  # create data directory for alloy
  - mkdir -p /var/lib/alloy/data
  # move loadgen script to home directory and make it runnable
  - mv /tmp/loadgen.sh /home/ubuntu/loadgen.sh
  - chown ubuntu:ubuntu /home/ubuntu/loadgen.sh
  - chmod 0755 /home/ubuntu/loadgen.sh
  
  
  # Download and install Grafana GPG key
  - curl -fsSL https://apt.grafana.com/gpg.key | gpg --dearmor -o /etc/apt/keyrings/grafana.gpg
  
  # Add Grafana repository
  - echo "deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main" > /etc/apt/sources.list.d/grafana.list
  
  # Update package lists
  - apt-get update

  # Install Alloy
  - DEBIAN_FRONTEND=noninteractive apt-get install -y alloy

  # Install Couchbase
  - |
    curl -O https://packages.couchbase.com/releases/couchbase-release/couchbase-release-1.0-noarch.deb
    dpkg -i ./couchbase-release-1.0-noarch.deb
    apt-get update
    apt-get install -y couchbase-server-community

  # initially wait for couchbase service to be ready
  - |
    timeout 15s bash <<EOF
    wait_for_couchbase() {
      until curl -s http://localhost:8091/ > /dev/null 2>&1; do
        echo "Waiting for Couchbase to be ready..."
        sleep 5
      done
    }

    wait_for_couchbase
    EOF

  # --- Configuring Alloy to run as root ---
  # Modify the service file to remove User and Group lines
  - sed -i '/^\[Service\]/,/^\[/ { /^[ \t]*User=/d; /^[ \t]*Group=/d }' /lib/systemd/system/alloy.service || echo "Could not modify /lib/systemd/system/alloy.service"
  # Also try modifying in /etc/systemd/system just in case
  - sed -i '/^\[Service\]/,/^\[/ { /^[ \t]*User=/d; /^[ \t]*Group=/d }' /etc/systemd/system/alloy.service || echo "Could not modify /etc/systemd/system/alloy.service"

  - systemctl daemon-reload
  - systemctl enable couchbase-loadgen
  - systemctl start couchbase-loadgen
  - systemctl enable alloy
  - systemctl restart alloy

  
  
