# Cloud-init configuration for setting up Alloy and required apache-airflow sample-app

apt:
  sources:
    grafana:
      source: deb https://apt.grafana.com stable main
      keyid: 963FA27710458545
      keyserver: https://apt.grafana.com/gpg.key

packages:
- git
- gpg
- alloy

write_files:
  # Alloy configuration
  - owner: root:root
    path: /etc/alloy/config.alloy
    content: |
      prometheus.exporter.self "alloy_check" { }

      discovery.relabel "alloy_check" {
        targets = prometheus.exporter.self.alloy_check.targets

        rule {
          target_label = "instance"
          replacement  = constants.hostname
        }

        rule {
          target_label = "alloy_hostname"
          replacement  = constants.hostname
        }

        rule {
          target_label = "job"
          replacement  = "integrations/alloy-check"
        }
      }

      prometheus.scrape "alloy_check" {
        targets    = discovery.relabel.alloy_check.output
        forward_to = [prometheus.relabel.alloy_check.receiver]  

        scrape_interval = "60s"
      }

      prometheus.relabel "alloy_check" {
        forward_to = [prometheus.remote_write.metrics_service.receiver]

        rule {
          source_labels = ["__name__"]
          regex         = "(prometheus_target_sync_length_seconds_sum|prometheus_target_scrapes_.*|prometheus_target_interval.*|prometheus_sd_discovered_targets|alloy_build.*|prometheus_remote_write_wal_samples_appended_total|process_start_time_seconds)"
          action        = "keep"
        }
      }

      prometheus.remote_write "metrics_service" {
        endpoint {
          url = "http://10.78.4.13:9009/api/v1/push"

          basic_auth {
            username = "your_prometheus_username"
            password = "your_prometheus_password"
          }
        }
      }

      loki.write "grafana_cloud_loki" {
        endpoint {
          url = "http://10.78.4.13:3100/loki/api/v1/push"

          basic_auth {
            username = "your_loki_username"
            password = "your_loki_password"
          }
        }
      }
      
      prometheus.exporter.statsd "integrations_statsd_exporter" {
        listen_udp = "localhost:8125"
        mapping_config_path = "/home/ubuntu/statsd_mapping.yaml"
      }

      discovery.relabel "integrations_statsd_exporter" {
        targets = prometheus.exporter.statsd.integrations_statsd_exporter.targets

        rule {
          target_label = "job"
          replacement  = "integrations/apache-airflow"
        }

        rule {
          target_label = "instance"
          replacement  = constants.hostname
        }
      }

      prometheus.scrape "integrations_statsd_exporter" {
        targets    = discovery.relabel.integrations_statsd_exporter.output
        forward_to = [prometheus.remote_write.metrics_service.receiver]
        job_name   = "integrations/statsd_exporter"
      }

      local.file_match "logs_integrations_integrations_apache_airflow" {
        path_targets = [
          {
            __address__ = "localhost",
            __path__    = "/home/airflow/airflow/logs/dag_id=*/**/*.log",
            instance    = constants.hostname,
            job         = "integrations/apache-airflow",
          },
          {
            __address__ = "localhost",
            __path__    = "/home/airflow/airflow/logs/scheduler/latest/*.py.log",
            instance    = constants.hostname,
            job         = "integrations/apache-airflow",
          },
        ]
      }

      loki.process "logs_integrations_integrations_apache_airflow" {
        forward_to = [loki.write.grafana_cloud_loki.receiver]

        stage.match {
          selector = format("{job=\"integrations/apache-airflow\",instance=\"%s\"}", constants.hostname)

          stage.regex {
            expression = "/home/airflow/airflow/logs/dag_id=(?P<dag_id>\\S+?)/.*/task_id=(?P<task_id>\\S+?)/.*log"
            source     = "filename"
          }

          stage.labels {
            values = {
              dag_id  = null,
              task_id = null,
            }
          }
        }

        stage.match {
          selector = format("{job=\"integrations/apache-airflow\",instance=\"%s\"}", constants.hostname)

          stage.regex {
            expression = "/home/airflow/airflow/logs/scheduler/latest/(?P<dag_file>\\S+?)\\.log"
            source     = "filename"
          }

          stage.labels {
            values = {
              dag_file = null,
            }
          }
        }

        stage.multiline {
          firstline     = "\\[\\d+-\\d+-\\d+T\\d+:\\d+:\\d+\\.\\d+\\+\\d+\\]"
          max_lines     = 0
          max_wait_time = "3s"
        }
      }

      loki.source.file "logs_integrations_integrations_apache_airflow" {
        targets    = local.file_match.logs_integrations_integrations_apache_airflow.targets
        forward_to = [loki.process.logs_integrations_integrations_apache_airflow.receiver]
      }

  - owner: root:root
    path: /home/ubuntu/install-airflow.sh
    content: |
      #!/bin/bash

      set -e

      # Pre-requisites
      # - Python 3.8+




      function wait_for_airflow {
          echo "Waiting for Airflow to be fully operational..."
          counter=0
          timeout=120
          while [ $counter -lt $timeout ]; do
              if sudo -u airflow bash -c "source $VENV_PATH/bin/activate && airflow dags list" > /dev/null 2>&1; then
                  break
              fi
              sleep 1
              counter=$((counter + 1))
          done
          if [ $counter -eq $timeout ]; then
              echo "Error: Airflow did not become operational within ${timeout} seconds"
              exit 1
          fi
      }

      AIRFLOW_VERSION="2.9.3"
      airflow_download_dir="$(mktemp -d)"

      sudo apt install -y python3.12 python3-pip python3.12-venv


      # Verify Python
      python3 --version
      if ! python3 -c "import sys; exit(0 if sys.version_info >= (3, 8) else 1)"; then
          echo "Python 3.8+ required"
          exit 1
      fi

      # Create airflow user first
      sudo useradd -m -s /bin/bash airflow

      # Create virtual environment with error checking
      VENV_PATH="${VENV_PATH:-/home/airflow/airflow-venv}"
      if [ ! -d "$VENV_PATH" ]; then
          sudo -u airflow python3 -m venv "$VENV_PATH"
      fi

      # Activate and upgrade pip
      sudo -u airflow bash -c "source $VENV_PATH/bin/activate && pip install --upgrade pip"

      # Verify constraint file exists before using
      PYTHON_VERSION="$(python3 -c 'import sys; print(f"{sys.version_info.major}.{sys.version_info.minor}")')"
      CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

      if curl --fail --silent --head "$CONSTRAINT_URL" > /dev/null; then
          sudo -u airflow bash -c "source $VENV_PATH/bin/activate && pip install 'apache-airflow['statsd']==${AIRFLOW_VERSION}' --constraint '${CONSTRAINT_URL}' --no-color"
      else
          echo "Warning: Constraint file not found, installing without constraints"
          sudo -u airflow bash -c "source $VENV_PATH/bin/activate && pip install 'apache-airflow['statsd']==${AIRFLOW_VERSION}' --no-color"
      fi

      # Install statsd client for metrics
      sudo -u airflow bash -c "source $VENV_PATH/bin/activate && pip install statsd --no-color"

      # Create initialization script
      sudo tee /home/airflow/init-airflow.sh > /dev/null << 'EOF'
      #!/bin/bash
      set -e

      VENV_PATH="/home/airflow/airflow-venv"
      export AIRFLOW_HOME="/home/airflow/airflow"

      # Initialize database
      source $VENV_PATH/bin/activate
      airflow db migrate

      # Create admin user (ignore if it already exists)
      airflow users create \
          --username admin \
          --firstname Airflow \
          --lastname Admin \
          --role Admin \
          --email admin@example.com \
          --password admin || echo "Admin user already exists"

      echo "Airflow initialized successfully"
      EOF

      sudo chmod +x /home/airflow/init-airflow.sh
      sudo chown airflow:airflow /home/airflow/init-airflow.sh

      # Create systemd service file for Airflow
      sudo tee /etc/systemd/system/airflow.service > /dev/null << 'EOF'
      [Unit]
      Description=Apache Airflow

      [Service]
      Type=simple
      User=airflow
      Group=airflow
      Environment=PATH=/home/airflow/airflow-venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
      Environment=AIRFLOW_HOME=/home/airflow/airflow
      WorkingDirectory=/home/airflow
      ExecStartPre=/home/airflow/init-airflow.sh
      ExecStart=/home/airflow/airflow-venv/bin/airflow standalone
      Restart=always
      RestartSec=10

      [Install]
      WantedBy=multi-user.target
      EOF

      # Reload systemd and enable the service to create all the configuration files
      sudo systemctl daemon-reload
      sudo systemctl enable airflow.service
      sudo systemctl start airflow.service

      wait_for_airflow

      # Modify airflow.cfg to enable StatsD metrics
      sudo sed -i 's/statsd_on = False/statsd_on = True/' /home/airflow/airflow/airflow.cfg
      sudo sed -i 's/statsd_prefix = airflow/statsd_prefix = airflow/' /home/airflow/airflow/airflow.cfg

      # Restart the service to apply the configuration change
      sudo systemctl restart airflow.service

      wait_for_airflow

      echo "Airflow installation completed!"


  - owner: root:root
    path: /home/ubuntu/statsd_mapping.yaml
    content: |
      # Licensed to the Apache Software Foundation (ASF) under one
      # or more contributor license agreements.  See the NOTICE file
      # distributed with this work for additional information
      # regarding copyright ownership.  The ASF licenses this file
      # to you under the Apache License, Version 2.0 (the
      # "License"); you may not use this file except in compliance
      # with the License.  You may obtain a copy of the License at
      #
      #   http://www.apache.org/licenses/LICENSE-2.0
      #
      # Unless required by applicable law or agreed to in writing,
      # software distributed under the License is distributed on an
      # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
      # KIND, either express or implied.  See the License for the
      # specific language governing permissions and limitations
      # under the License.
      ---
      mappings:
      - match: "airflow.dag.*.*.duration"
        match_metric_type: observer
        name: "airflow_dag_task_duration"
        labels:
          dag_id: "$1"
          task_id: "$2"
      - match: "airflow.dagrun.duration.success.*"
        match_metric_type: observer
        name: "airflow_dagrun_duration_success"
        labels:
          dag_id: "$1"
      - match: "airflow.dagrun.duration.failed.*"
        match_metric_type: observer
        name: "airflow_dagrun_duration_failed"
        labels:
          dag_id: "$1"
      - match: "airflow.ti.start.*.*"
        match_metric_type: counter
        name: "airflow_task_start_total"
        labels:
          dag_id: "$1"
          task_id: "$2"
      - match: "airflow.ti.finish.*.*.*"
        match_metric_type: counter
        name: "airflow_task_finish_total"
        labels:
          dag_id: "$1"
          task_id: "$2"
          state: "$3"
      - match: "airflow.dagrun.schedule_delay.*"
        match_metric_type: observer
        name: "airflow_dagrun_schedule_delay"
        labels:
          dag_id: "$1"
      - match: "airflow.pool.running_slots.*"
        match_metric_type: gauge
        name: "airflow_pool_running_slots"
        labels:
          pool_name: "$1"
      - match: "airflow.pool.queued_slots.*"
        match_metric_type: gauge
        name: "airflow_pool_queued_slots"
        labels:
          pool_name: "$1"
      - match: "airflow.pool.open_slots.*"
        match_metric_type: gauge
        name: "airflow_pool_open_slots"
        labels:
          pool_name: "$1"
      - match: "airflow.pool.starving_tasks.*"
        match_metric_type: gauge
        name: "airflow_pool_starving_tasks"
        labels:
          pool_name: "$1"


  - owner: root:root
    path: /home/ubuntu/loadgen.sh
    content: |
      #!/bin/bash

      set -e

      VENV_PATH="/home/airflow/airflow-venv"

      echo "Waiting for Airflow to be fully operational..."
      timeout=120
      counter=0
      while [ $counter -lt $timeout ]; do
          if sudo -u airflow bash -c "source $VENV_PATH/bin/activate && airflow dags list" > /dev/null 2>&1; then
              echo "Airflow is operational!"
              break
          fi
          echo "Waiting for Airflow... ($counter/$timeout seconds)"
          sleep 5
          counter=$((counter + 5))
      done

      if [ $counter -ge $timeout ]; then
          echo "Error: Airflow did not become operational within ${timeout} seconds"
          exit 1
      fi

      echo "Starting continuous DAG triggering..."

      while true; do 
          echo "Checking for available DAGs..."
          available_dags=$(sudo -u airflow bash -c "source $VENV_PATH/bin/activate && airflow dags list --output plain" 2>/dev/null | tail -n +2)
          
          if [ -n "$available_dags" ]; then
              # Get the first available DAG
              first_dag=$(echo "$available_dags" | head -n 1 | awk '{print $1}')
              echo "Triggering DAG run for: $first_dag"
              
              if sudo -u airflow bash -c "source $VENV_PATH/bin/activate && airflow dags trigger $first_dag" > /dev/null 2>&1; then
                  echo "DAG run triggered successfully!"
              else
                  echo "Failed to trigger DAG run for: $first_dag"
              fi
          else
              echo "No DAGs found to trigger"
          fi
          
          echo "Waiting 30 seconds before next trigger..."
          sleep 30
      done 


  - owner: root:root
    path: /etc/systemd/system/loadgen.service
    content: |
      [Unit]
      Description=Apache Airflow Load Generator
      After=airflow.service
      Requires=airflow.service

      [Service]
      Type=simple
      User=root
      Group=root
      WorkingDirectory=/home/ubuntu
      ExecStart=/home/ubuntu/loadgen.sh
      Restart=always
      RestartSec=30
      StandardOutput=journal
      StandardError=journal

      [Install]
      WantedBy=multi-user.target


runcmd:
  # General setup
  - sudo apt-get update
  - chmod +x /home/ubuntu/install-airflow.sh
  - chmod +x /home/ubuntu/loadgen.sh

  # Installs Apache Airflow and creates the admin user
  - sudo /home/ubuntu/install-airflow.sh > /home/ubuntu/airflow-install.log 2>&1

  # Give alloy user access to airflow logs
  - sudo usermod -a -G airflow alloy || echo "Could not add alloy to airflow group"

  # Start Alloy
  - sudo systemctl daemon-reload
  - sudo systemctl enable alloy.service
  - sudo systemctl start alloy.service

  # Start placed loadgen service
  - sudo systemctl enable loadgen.service
  - sudo systemctl start loadgen.service
