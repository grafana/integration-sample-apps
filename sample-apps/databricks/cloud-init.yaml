#cloud-config
# Cloud-init configuration for setting up Alloy and Databricks exporter sample-app

package_update: true
package_upgrade: false

packages:
  - git
  - gpg
  - curl
  - wget

write_files:
  # Alloy profile
  - owner: root:root
    path: /etc/default/alloy
    content: |
      ## Path:
      ## Description: Grafana Alloy settings
      ## Type:        string
      ## Default:     ""
      ## ServiceRestart: alloy
      #
      # Command line options for Alloy.
      #
      # The configuration file holding the Alloy config.
      CONFIG_FILE="/etc/alloy/config.alloy"
      # User-defined arguments to pass to the run command.
      CUSTOM_ARGS="--stability.level=experimental"
      # Restart on system upgrade. Defaults to true.
      RESTART_ON_UPGRADE=true

  # Alloy configuration
  - owner: root:root
    path: /etc/alloy/config.alloy
    content: |
      // Alloy self-monitoring
      prometheus.exporter.self "alloy_check" { }

      discovery.relabel "alloy_check" {
        targets = prometheus.exporter.self.alloy_check.targets
        rule {
          target_label = "instance"
          replacement  = constants.hostname
        }
        rule {
          target_label = "alloy_hostname"
          replacement  = constants.hostname
        }
        rule {
          target_label = "job"
          replacement  = "integrations/alloy-check"
        }
      }

      prometheus.scrape "alloy_check" {
        targets         = discovery.relabel.alloy_check.output
        forward_to      = [prometheus.relabel.alloy_check.receiver]
        scrape_interval = "60s"
      }

      prometheus.relabel "alloy_check" {
        forward_to = [prometheus.remote_write.metrics_service.receiver]
        rule {
          source_labels = ["__name__"]
          regex         = "(prometheus_target_sync_length_seconds_sum|prometheus_target_scrapes_.*|prometheus_target_interval.*|prometheus_sd_discovered_targets|alloy_build.*|prometheus_remote_write_wal_samples_appended_total|process_start_time_seconds)"
          action        = "keep"
        }
      }

      // Databricks exporter scraping
      // The exporter runs as a Docker container on localhost:9976
      prometheus.scrape "integrations_databricks" {
        targets = [{
          __address__ = "localhost:9976",
        }]
        forward_to      = [prometheus.relabel.integrations_databricks.receiver]
        scrape_interval = "5m"
        scrape_timeout  = "4m"
        job_name        = "integrations/databricks"
      }

      prometheus.relabel "integrations_databricks" {
        forward_to = [prometheus.remote_write.metrics_service.receiver]
        rule {
          target_label = "instance"
          replacement  = constants.hostname
        }
        rule {
          target_label = "job"
          replacement  = "integrations/databricks"
        }
      }

      prometheus.remote_write "metrics_service" {
        endpoint {
          url = "http://your-prometheus-instance:9090/api/v1/push"
          basic_auth {
            username = "your_prometheus_username"
            password = "your_prometheus_password"
          }
        }
      }

      // Docker log discovery for the databricks-exporter container
      discovery.docker "databricks_exporter" {
        host = "unix:///var/run/docker.sock"
        refresh_interval = "5s"
        filter {
          name = "name"
          values = ["databricks-exporter"]
        }
      }

      discovery.relabel "databricks_exporter" {
        targets = discovery.docker.databricks_exporter.targets
        rule {
          source_labels = ["__meta_docker_container_name"]
          target_label  = "name"
          replacement   = "databricks-exporter"
        }
        rule {
          source_labels = ["__meta_docker_container_name"]
          target_label  = "job"
          replacement   = "integrations/databricks"
        }
        rule {
          source_labels = ["__meta_docker_container_name"]
          target_label  = "instance"
          replacement   = constants.hostname
        }
      }

      loki.source.docker "databricks_exporter" {
        host = "unix:///var/run/docker.sock"
        targets = discovery.docker.databricks_exporter.targets
        forward_to = [loki.write.grafana_cloud_loki.receiver]
        relabel_rules = discovery.relabel.databricks_exporter.rules
      }

      loki.write "grafana_cloud_loki" {
        endpoint {
          url = "http://your-loki-instance:3100/loki/api/v1/push"
          basic_auth {
            username = "your_loki_username"
            password = "your_loki_password"
          }
        }
      }

runcmd:
  - mkdir -p /home/ubuntu
  - mkdir -p /etc/apt/keyrings/
  # Create required directory for alloy
  - mkdir -p /var/lib/alloy
  - chown -R root:root /var/lib/alloy

  # Install Grafana repo
  - curl -fsSL https://apt.grafana.com/gpg.key | gpg --dearmor -o /etc/apt/keyrings/grafana.gpg
  - echo "deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main" > /etc/apt/sources.list.d/grafana.list

  # Install Docker repo
  - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
  - echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

  - apt-get update
  - DEBIAN_FRONTEND=noninteractive apt-get install -y alloy docker-ce docker-ce-cli containerd.io

  # Add ubuntu user to docker group
  - groupadd -f docker
  - usermod -aG docker ubuntu
  - chmod 666 /var/run/docker.sock || true
  - systemctl restart docker
  - systemctl enable docker
  - systemctl start docker

  # Run the Databricks exporter container
  # NOTE: The exporter queries System Tables which can take 2-4 minutes
  # Scrape interval should be 5m with a 4m timeout to avoid overlapping scrapes
  - |
    docker run -d \
      --name databricks-exporter \
      --restart unless-stopped \
      -p 9976:9976 \
      -e DATABRICKS_EXPORTER_SERVER_HOSTNAME="your-workspace.cloud.databricks.com" \
      -e DATABRICKS_EXPORTER_WAREHOUSE_HTTP_PATH="/sql/1.0/warehouses/your-warehouse-id" \
      -e DATABRICKS_EXPORTER_CLIENT_ID="your-service-principal-client-id" \
      -e DATABRICKS_EXPORTER_CLIENT_SECRET="your-service-principal-client-secret" \
      ghcr.io/grafana/databricks-prometheus-exporter:latest

  # Wait for the exporter to start
  - |
    for i in {1..30}; do
      if curl -s http://localhost:9976/metrics > /dev/null 2>&1; then
        echo "Databricks exporter is ready"
        break
      fi
      echo "Waiting for Databricks exporter to start..."
      sleep 2
    done

  # Configure Alloy to run as root (needed for Docker socket access)
  - sed -i '/^\[Service\]/,/^\[/ { /^[ \t]*User=/d; /^[ \t]*Group=/d }' /lib/systemd/system/alloy.service || echo "Could not modify /lib/systemd/system/alloy.service"
  - sed -i '/^\[Service\]/,/^\[/ { /^[ \t]*User=/d; /^[ \t]*Group=/d }' /etc/systemd/system/alloy.service || echo "Could not modify /etc/systemd/system/alloy.service"

  - systemctl daemon-reload
  - systemctl enable alloy
  - systemctl restart alloy

